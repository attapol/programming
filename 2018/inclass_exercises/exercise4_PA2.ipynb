{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "List, tokenization, and while loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - Prefix\n",
    "คำกริยาบางคำในภาษาอังกฤษสามารถเติม prefix ได้\n",
    "\n",
    "จงเขียน function ที่รับ list of verbs และ return list of verbs + verbs ที่มี prefix ที่ระบุไว้ต่อหน้า"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(verb_list, prefix):\n",
    "    \"\"\"Add prefix to all verbs\n",
    "    \n",
    "    >>> add_prefix(['start', 'gain'], 're')\n",
    "    ['start', 'gain', 'restart', 'regain']\n",
    "    \n",
    "    >>> add_prefix(['produce', 'estimate', 'do'], 'over')\n",
    "    ['produce', 'estimate', 'do', 'overproduce', 'overestimate', 'overdo']\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - Hailstone sequence\n",
    "จำนวนเต็ม n ที่ค่ามากกว่า 1 สามารถทำให้กลายเป็น 1 ด้วยกระบวนการดังต่อไปนี้\n",
    "\n",
    "ทำไปเรื่อยๆ จนกว่า n จะมีค่าเท่ากับ 1 \n",
    "\n",
    "* ถ้า n เป็นเลขคี่ ให้คูณสามและบวกหนึ่ง\n",
    "* ถ้า n เป็นเลขคู่ ให้หารสอง\n",
    "\n",
    "เช่น 5 --> 16 --> 8 --> 4 --> 2 --> 1 \n",
    "ตัวเลขเหล่านี้เรียกว่า Hailstone sequence\n",
    "\n",
    "จงเขียน function ที่ผลิต Hailstone sequence จากตัวเลขที่กำหนดให้โดยให้ return เป็น list ของตัวเลข และ print กระบวนการออกมาตามตัวอย่าง จากนั้นลองใช้ฟังก์ชันกับ 5 และ 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hailstone(n):\n",
    "    \"\"\"Generate a Hailstone sequence from a given number\n",
    "    \n",
    "    >>> hailstone(n)\n",
    "    5 is odd, so I make 3n + 1 = 16\n",
    "    \n",
    "    \n",
    "    16 is even, so I make 16 / 2 = 8\n",
    "    8 is even, so I make 8 / 2 = 4\n",
    "    4 is even, so I make 4 / 2 = 2\n",
    "    2 is even, so I make 2 / 2 = 1\n",
    "    [5, 16, 8, 4, 2, 1]\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "hailstone(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 - Generate Trigram\n",
    "Trigram เป็นหน่วยโกงๆ ของการวิเคราะห์ประโยคในแบบฉบับของ NLP กล่าวคือวิเคราะห์ว่าสามคำที่อยู่ติดกันนั้นเป็นคำเดียวกัน เช่น ประโยค\n",
    "\n",
    "\"It rains in the evening\"  \n",
    "\n",
    "ประกอบด้วย trigram ดังต่อไปนี้\n",
    "It_rains_in\n",
    "rains_in_the\n",
    "in_the_evening\n",
    "\n",
    "Trigram เป็นตัวบ่งบอกว่าคำไหนพบเจอกับคำไหนบ่อย เช่น in the evening อาจจะเป็นคำที่เจอบ่อยมากๆ เพราะว่าค่อนข้าง idiomatic ทำหน้าที่คล้ายๆ เหมือนคำเดียวแต่เราเขียนออกมาเป็นสามคำแยกกัน\n",
    "\n",
    "จงเขียน function ที่ผลิต trigram โดยให้ return list of trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trigram(raw_string):\n",
    "    \"\"\"Generate trigram from an untokenized sentence\n",
    "    \n",
    "    >>> generate_trigram(\"It rains in the evening\")\n",
    "    [\"It_rains_in\", \"rains_in_the\", \"in_the_evening\"]\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 - Creating a lexicon from data\n",
    "จงเขียน function ที่เปลี่ยนข้อมูลที่เป็น string ให้เป็นรายการคำศัพท์ (lexicon) ที่เรียงลำดับจาก a-z และ lexicon จะต้องมีลักษณะดังนี้\n",
    "\n",
    "1. จะต้องไม่มีตัวเลขปนอยู่ เช่น 34 45,00 4K\n",
    "2. แต่ละคำจะต้องไม่มี . หรือ , ติดอยู่กับคำ\n",
    "3. ใน lexicon ห้ามมีคำซ้ำ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['worda', 'wordb']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_lexicon(untokenized_string):\n",
    "    \"\"\"\n",
    "    \n",
    "    >>> build_lexicon(\"In the evening, he burns another 1,000 calories like every evening\")\n",
    "    ['In', 'another', 'burns', 'calories', 'evening', 'every', 'he', 'like', 'the']\n",
    "    \n",
    "    >>> build_lexicon(\"worda wordb, worda worda 123 9999. \")\n",
    "    ['worda', 'wordb']\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "build_lexicon(\"In the evening, he burns another 1,000 calories like every evening\")\n",
    "build_lexicon(\"worda wordb, 2k wordb wordb 123 9999. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 2 - Rule-based NLP Preprocessing Suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence splitter (2 คะแนน)\n",
    "ประโยคในภาษาอังกฤษมักถูกขั้นด้วยจุด full stop แต่ปัญหาอยู่ที่ว่าจุดบางจุดไม่มีเป็นตัวแบ่งประโยคเช่น \n",
    "\n",
    "Mr. Davidson climbed Mt. Fuji before finishing his Ph.D. in Tokyo. And then something bad happened...\n",
    "\n",
    "จงเขียน function ที่เปลี่ยน string เป็น list of sentences โดยที่ต้องคำนึงถึงจุด full stop ที่ไม่ได้เป็นตัวแบ่งประโยค จากนั้นให้ทดสอบกับประโยค 3-4 ประโยคเพื่อแสดงให้เห็นว่า sentences splitter นั้นสามารถทำงานได้ดีพอสมควร "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_split(unsplit_text):\n",
    "    \"\"\"Split a piece of text into a list of sentences\n",
    "    \n",
    "    >>> sentence_split(\"\"\"Mr. Davidson climbed Mt. Fuji before \n",
    "    finishing his Ph.D. in Tokyo. And then something nice happened... His wife got pregnant.\"\"\")\n",
    "    ['Mr. Davidson climbed Mt. Fuji before finishing his Ph.D. in Tokyo.', \n",
    "    'And then something nice happened...',\n",
    "    'His wife got pregnant.']\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer (2 คะแนน)\n",
    "วิธีการตัดคำ (tokenization/word segmentation) สามารถทำได้แบบฉาบฉวยคือใช้ method `S.split(' ')` วิธีนี้มีข้อบกพร่องตรงที่ว่า \n",
    "\n",
    "1. Punctuation ไม่ถูกแยกออกเป็นอีก token หนึ่ง เช่น 'Aaron, Bill, and Catherine.' ควรจะเป็น `['Aaron', ',', 'Bill', ',', 'and', 'Catherine', '.']`\n",
    "2. Hyphenated adjectives ไม่ถูกแยกออกเป็น 3 tokens เช่น 'fire-roasted salmon' ควรจะเป็น `['fire', '-', 'roasted', 'salmon']`\n",
    "\n",
    "จงเขียน tokenizer ที่ไม่มีข้อบกพร่องดังกล่าว"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(untokenized_text):\n",
    "    \"\"\" Tokenize text in a bit more sophisticated way.\n",
    "    \n",
    "    >>> tokenizer('Aaron, Bill, and Catherine.')\n",
    "    ['Aaron', ',', 'Bill', ',', 'and', 'Catherine', '.']\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based part-of-speech tagger (6 คะแนน)\n",
    "ชนิดของคำ (Part-of-speech tag) เป็นส่วนประกอบที่สำคัญในการวิเคราะห์คำ และการวิเคราะห์ประโยค (parsing) POS tags ที่ใช้กันอย่างกว้างขวางในวงการ NLP คือ [Universal POS tags](http://universaldependencies.org/u/pos/all.html) ที่สามารถใช้วิเคราะห์ประโยคได้เกือบทุกภาษาทั่วโลก Automatic part-of-speech tagger ในปัจจุบันนั้นอาศัยข้อมูลจำนวนมากเพื่อฝึก machine learning model แต่ที่จริงเราสามารถเขียนระบบอิงกฏ (rule-based system) ในการเติม part-of-speech tag ให้กับคำแต่ละคำในประโยค\n",
    "\n",
    "จงเขียน function ที่เปลี่ยน list of words เป็น list of part-of-speech tags จากนั้นให้ทดสอบกับประโยค 4-5 ประโยคเพื่อแสดงให้เห็นว่า POS tag ของเรานั้นสามารถทำงานได้ดีในสถานการณ์ใดบ้าง\n",
    "\n",
    "Tagger ที่เขียนจะต้องสามารถใช้ POS Tags ต่อไปนี้\n",
    "ADJ\n",
    "ADV\n",
    "NOUN\n",
    "PROPN\n",
    "VERB\n",
    "ADP\n",
    "AUX\n",
    "DET\n",
    "NUM\n",
    "PRON\n",
    "PUNCT\n",
    "\n",
    "Tagger จะต้องใช้กฏหยาบๆ ฉาบฉวย (heuristics) เช่น คำขึ้นต้นตัวใหญ่มักจะเป็น PROPN เป็นต้น "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(word_list):\n",
    "    \"\"\"Tag tokenized text\n",
    "    \n",
    "    >>> pos_tag([\"She\", \"criticized\", \"Aaron\", \".\"])\n",
    "    [\"PRON\", \"VERB\", \"PROPN\", \"PUNCT\"]\n",
    "    \n",
    "    >>> pos_tag([\"The\", \"reality\", \"hit\", \"me\", \"quickly\", \".\"])\n",
    "    [\"DET\", \"NOUN\", \"VERB\", \"PRON\", \"ADV\", \"PUNCT\"]\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
